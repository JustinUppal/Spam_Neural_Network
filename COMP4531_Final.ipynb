{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "34fe1da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52db25f6",
   "metadata": {},
   "source": [
    "I am going to try building a model with just a subset of the data. Then I will build the entire model.\n",
    "This is so I can first make sure I have everything working properly before I take all the computational\n",
    "power to train the model on a large dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6225d5fa",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72bf2d94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pu_corpora_public', '.DS_Store']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "I'm extracting the tar file and\n",
    "grabbing the contents of everything I've extracted.\n",
    "We also need to make sure the extraction file exists\n",
    "\n",
    "'''\n",
    "tar_path = 'pu_corpora_public.tar'\n",
    "extract_to = 'pu_corpora_public'\n",
    "\n",
    "if not os.path.exists(extract_to):\n",
    "    os.makedirs(extract_to)\n",
    "\n",
    "with tarfile.open(tar_path) as tar:\n",
    "    tar.extractall(path=extract_to)\n",
    "\n",
    "extracted_contents = os.listdir(extract_to)\n",
    "extracted_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b20ab18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pu1', 'pua', 'readme.txt', 'pu2', 'pu3']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "Now I am grabbing all the files\n",
    "of the dataset + readme and listing them \n",
    "'''\n",
    "\n",
    "subdirectory_path = os.path.join(extract_to, 'pu_corpora_public')\n",
    "subdirectory_contents = os.listdir(subdirectory_path)\n",
    "subdirectory_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5aa8d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This directory contains the PU1, PU2, PU3, and PUA corpora, as \n",
      "described in the paper:\n",
      "\n",
      "I. Androutsopoulos, G. Paliouras, E. Michelakis, \"Learning to \n",
      "Filter Unsolicited Commercial E-Mail\", submitted for journal \n",
      "publication, 2003.\n",
      "\n",
      "There are 4 directories (pu1, pu2, pu3, pua), each containing\n",
      "one of the four corpora. \n",
      "\n",
      "Each one of the 4 directories in turn contains 11 subdirectories \n",
      "(part1, ..., part10, unused). These correspond to the 10 partitions \n",
      "of each corpus that were used in the 10-fo\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Now I am examining the readme file\n",
    "\n",
    "'''\n",
    "readme_path = os.path.join(subdirectory_path, 'readme.txt')\n",
    "\n",
    "with open(readme_path, 'r') as file:\n",
    "    readme_contents = file.read()\n",
    "\n",
    "print(readme_contents[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1854616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1988legit13.txt',\n",
       " '1221legit54.txt',\n",
       " '1198legit14.txt',\n",
       " '1394spmsg90.txt',\n",
       " '1110legit57.txt',\n",
       " '1545legit49.txt',\n",
       " '177spmsg68.txt',\n",
       " '1716legit2.txt',\n",
       " '1926spmsg88.txt',\n",
       " '1489legit32.txt']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Now lets a look at one of the subdirectories\n",
    "in on of the corpora to see what the data\n",
    "formatting looks like\n",
    "\n",
    "Notice that emails that are not spam are considered \"legit\"\n",
    "'''\n",
    "\n",
    "\n",
    "sample_corpus_path = os.path.join(subdirectory_path, 'pu1', 'part1')\n",
    "\n",
    "\n",
    "sample_corpus_contents = os.listdir(sample_corpus_path)\n",
    "sample_corpus_contents[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d66e3c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: 5573 47\n",
      "\n",
      "3677 22660 15981 9594 5573 2130 16502 22064 15981 9594 84 19054 9594 16893 7913 1613 16502 8615 3617 1991 3677 22660 80 4695 19054 12995 8890 84 16502 9594 7602 6217 1967 16502 17157 1991 16502 15779 20385 15981 9594 80 14911 897 16993 19283 18957 3617 80 14338 180 631 1967 180 2521 14766 15820 4978 22328 84 19054 15981 9594 1847 17912 2130 1594 180 18316 20215 23479 14338 7088 6485 1835 3677 22660 84 19889 1967 19410 18065 1594 23772 2130 51 22064 132 1672 84 7423 84 14000 51 \n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "I'm going to take a look at one of the text files.\n",
    "I'm writing a function to read these and better examine them\n",
    "\n",
    "Interesting finds here:\n",
    "\n",
    "Notice that everything is already tokenized.\n",
    "This dataset has already been pre-processed to some extent\n",
    "\n",
    "'''\n",
    "\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        return file.read()\n",
    "sample_file_path = os.path.join(sample_corpus_path, sample_corpus_contents[0])\n",
    "sample_file_content = load_text_file(sample_file_path)\n",
    "print(sample_file_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b31b02",
   "metadata": {},
   "source": [
    "Here is my pre-processing plan:\n",
    "\n",
    "Since subject is in every email, we can remove the word \"subject\" from each line.\n",
    "\n",
    "The text is already tokenized but I think\n",
    "we need to convert text to sequences of tokens or embeddings\n",
    "when working from raw text like this\n",
    "\n",
    "Lastly:\n",
    "\n",
    "We can determine the labels on the emails (spam/not spam) based on the file-name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "54b7a8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "this function will determine if a file is spam based on its filename\n",
    "'''\n",
    "def is_spam(filename):\n",
    "    return 'spmsg' in filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fdc32672",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function will load files from a directory\n",
    "then return a DataFrame with content and labels\n",
    "\n",
    "'''\n",
    "\n",
    "def load_data_from_directory(directory_path, sample_size_per_category=100):\n",
    "    files = os.listdir(directory_path)\n",
    "    contents = []\n",
    "    labels = []\n",
    "    \n",
    "    spam_count = 0\n",
    "    legit_count = 0\n",
    "    \n",
    "    for file in files:\n",
    "        if spam_count >= sample_size_per_category and legit_count >= sample_size_per_category:\n",
    "            break\n",
    "        \n",
    "        file_path = os.path.join(directory_path, file)\n",
    "        content = load_text_file(file_path)\n",
    "        \n",
    "        content = re.sub(r'Subject:.*\\n?', '', content, count=1)\n",
    "        \n",
    "        if is_spam(file) and spam_count < sample_size_per_category:\n",
    "            contents.append(content)\n",
    "            labels.append(1)\n",
    "            spam_count += 1\n",
    "        elif not is_spam(file) and legit_count < sample_size_per_category:\n",
    "            contents.append(content)\n",
    "            labels.append(0)\n",
    "            legit_count += 1\n",
    "    \n",
    "    return pd.DataFrame({'content': contents, 'label': labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f911b1eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n3677 22660 15981 9594 5573 2130 16502 22064 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n1791 13383 80 8962 2130 15184 17345 9131 217...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n1835 23758 17345 16531 16502 7634 17753 2040...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n22180 11245 14338 2649 13406 1124 47 47 47 4...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n82 82 82 82 82 82 82 82 82 82 82 82 82 82 82...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  label\n",
       "0  \\n3677 22660 15981 9594 5573 2130 16502 22064 ...      0\n",
       "1  \\n1791 13383 80 8962 2130 15184 17345 9131 217...      0\n",
       "2  \\n1835 23758 17345 16531 16502 7634 17753 2040...      0\n",
       "3  \\n22180 11245 14338 2649 13406 1124 47 47 47 4...      1\n",
       "4  \\n82 82 82 82 82 82 82 82 82 82 82 82 82 82 82...      0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading a balanced sample dataset\n",
    "sample_data = load_data_from_directory(sample_corpus_path)\n",
    "sample_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fe6058f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109, 2)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8c4748",
   "metadata": {},
   "source": [
    "\n",
    "Now we want to split the data into training and test sets.\n",
    "\n",
    "There is a problem of unequal shapes of the sequences when trying\n",
    "to convert the text data directly into numpy arrays.\n",
    "This is because the emails (now sequences of numbers) are unequal in length,\n",
    "which is a common issue in text data processing.\n",
    "\n",
    "We can fix this by padding the sequences so they have a uniform length.\n",
    "This is a requirment for feeding them into most neural networks.\n",
    "Let's find a good sequence length by looking at\n",
    "the distribution of lengths in our dataset\n",
    "and then we can pad the sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dca9c586",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sq/sf7z67k14rz104b31mjqffpm0000gn/T/ipykernel_21576/1961154175.py:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_train = np.array([np.fromstring(x, sep=' ') for x in X_train])\n",
      "/var/folders/sq/sf7z67k14rz104b31mjqffpm0000gn/T/ipykernel_21576/1961154175.py:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_val = np.array([np.fromstring(x, sep=' ') for x in X_val])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((87,), (22,))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "I'm splitting the dataset into training and validation sets,\n",
    "then we convert the sets into numpy arrays for tensorflow/\n",
    "\n",
    "'''\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(sample_data['content'], sample_data['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "X_train = np.array([np.fromstring(x, sep=' ') for x in X_train])\n",
    "X_val = np.array([np.fromstring(x, sep=' ') for x in X_val])\n",
    "\n",
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "17fa6a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': 920.183486238532,\n",
       " 'median': 453.0,\n",
       " 'max': 12862,\n",
       " 'min': 10,\n",
       " 'std': 1596.6688153096563}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Getting the sequence lengths then figuring out a reasonable padding length\n",
    "I'm printing out the basic stats to help figure it out.\n",
    "\n",
    "'''\n",
    "\n",
    "sequence_lengths = [len(np.fromstring(x, sep=' ')) for x in sample_data['content']]\n",
    "\n",
    "sequence_length_stats = {\n",
    "    'mean': np.mean(sequence_lengths),\n",
    "    'median': np.median(sequence_lengths),\n",
    "    'max': np.max(sequence_lengths),\n",
    "    'min': np.min(sequence_lengths),\n",
    "    'std': np.std(sequence_lengths)\n",
    "}\n",
    "\n",
    "sequence_length_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ddd6a5de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((87, 500), (22, 500))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "I'm going to set our sequence lengths to 500.\n",
    "This is slightly above the median\n",
    "\n",
    "'''\n",
    "\n",
    "sequence_length = 500\n",
    "\n",
    "X_train_padded = pad_sequences(X_train, maxlen=sequence_length, padding='post', truncating='post')\n",
    "X_val_padded = pad_sequences(X_val, maxlen=sequence_length, padding='post', truncating='post')\n",
    "\n",
    "X_train_padded.shape, X_val_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a9acbee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:81: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_14 (\u001b[38;5;33mEmbedding\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │     \u001b[38;5;34m1,280,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m33,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,313,089</span> (5.01 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,313,089\u001b[0m (5.01 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,313,089</span> (5.01 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,313,089\u001b[0m (5.01 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "Here are the model parameters\n",
    "I am adjusting this based on the\n",
    "dataset's vocabulary and\n",
    "the size of the vectors.\n",
    "\n",
    "'''\n",
    "\n",
    "vocab_size = 20000\n",
    "embedding_dim = 64\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_shape=(sequence_length,)),\n",
    "    LSTM(64, return_sequences=False),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "54146e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 160ms/step - accuracy: 0.4312 - loss: 0.6942 - val_accuracy: 0.4545 - val_loss: 0.6932\n",
      "Epoch 2/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.6091 - loss: 0.6880 - val_accuracy: 0.6364 - val_loss: 0.6849\n",
      "Epoch 3/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.6867 - loss: 0.6782 - val_accuracy: 0.6364 - val_loss: 0.6792\n",
      "Epoch 4/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.6656 - loss: 0.6637 - val_accuracy: 0.6364 - val_loss: 0.6716\n",
      "Epoch 5/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.6617 - loss: 0.6487 - val_accuracy: 0.6364 - val_loss: 0.6637\n",
      "Epoch 6/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.7081 - loss: 0.6241 - val_accuracy: 0.6364 - val_loss: 0.6553\n",
      "Epoch 7/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.6649 - loss: 0.6013 - val_accuracy: 0.6364 - val_loss: 0.6496\n",
      "Epoch 8/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.7237 - loss: 0.5429 - val_accuracy: 0.4545 - val_loss: 0.7100\n",
      "Epoch 9/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.7254 - loss: 0.5509 - val_accuracy: 0.5000 - val_loss: 0.8439\n",
      "Epoch 10/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.7293 - loss: 0.6170 - val_accuracy: 0.4545 - val_loss: 0.8726\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Training the model on padded data\n",
    "\n",
    "'''\n",
    "\n",
    "history = model.fit(X_train_padded, y_train, epochs=10, validation_data=(X_val_padded, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0df0f0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.4545 - loss: 0.8726\n",
      "Validation Accuracy: 45.45%\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "evaluating the model\n",
    "\n",
    "'''\n",
    "\n",
    "loss, accuracy = model.evaluate(X_val_padded, y_val)\n",
    "print(f\"Validation Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d5fc77",
   "metadata": {},
   "source": [
    "This is model above just a sample of the data. There really isn't enough data to build a reasonable model by just\n",
    "using a sample. So now I am going to use the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "379494e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                             content  label\n",
       " 0  \\n284 6818 80 284 13383 80 127 93 84 489 18798...      0\n",
       " 1  \\n16502 21946 7634 16893 15149 1613 16538 80 1...      0\n",
       " 2  \\n14338 7488 2221 20439 103 80 12116 18469 187...      0\n",
       " 3  \\n4822 80 16502 2410 1967 180 262 1847 12146 1...      1\n",
       " 4  \\n478 17188 130 20259 12808 80 196 17054 1812 ...      1,\n",
       " (7101, 2))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "This is a function written in order to\n",
    "load all of the data within my dataset\n",
    "instead of just a sample\n",
    "'''\n",
    "\n",
    "def load_all_data(directory_path):\n",
    "    all_contents = []\n",
    "    all_labels = []\n",
    "\n",
    "    for corpus_dir in ['pu1', 'pu2', 'pu3', 'pua']:\n",
    "        corpus_path = os.path.join(directory_path, corpus_dir)\n",
    "\n",
    "        for part in os.listdir(corpus_path):\n",
    "            part_path = os.path.join(corpus_path, part)\n",
    "            \n",
    "            if not os.path.isdir(part_path):\n",
    "                continue\n",
    "            \n",
    "            for file in os.listdir(part_path):\n",
    "                file_path = os.path.join(part_path, file)\n",
    "                content = load_text_file(file_path)\n",
    "                \n",
    "                content = re.sub(r'Subject:.*\\n?', '', content, count=1)\n",
    "\n",
    "                all_contents.append(content)\n",
    "                all_labels.append(1 if 'spmsg' in file else 0)\n",
    "\n",
    "\n",
    "    return pd.DataFrame({'content': all_contents, 'label': all_labels})\n",
    "\n",
    "\n",
    "entire_dataset = load_all_data(subdirectory_path)\n",
    "entire_dataset.head(), entire_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7b8691fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sq/sf7z67k14rz104b31mjqffpm0000gn/T/ipykernel_21576/569267593.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X = np.array([np.fromstring(text, sep=' ') for text in entire_dataset['content']])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Building the numpy arrays for the entire dataset\n",
    "\n",
    "'''\n",
    "\n",
    "X = np.array([np.fromstring(text, sep=' ') for text in entire_dataset['content']])\n",
    "y = entire_dataset['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4fc7fc9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': 673.6744120546402,\n",
       " 'median': 290.0,\n",
       " 'max': 134874,\n",
       " 'min': 1,\n",
       " 'std': 2313.5984150545423}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_lengths = [len(np.fromstring(x, sep=' ')) for x in entire_dataset['content']]\n",
    "\n",
    "sequence_length_stats = {\n",
    "    'mean': np.mean(sequence_lengths),\n",
    "    'median': np.median(sequence_lengths),\n",
    "    'max': np.max(sequence_lengths),\n",
    "    'min': np.min(sequence_lengths),\n",
    "    'std': np.std(sequence_lengths)\n",
    "}\n",
    "\n",
    "sequence_length_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9431e789",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Using padding the ensure uniform length\n",
    "Using 300, since that is slightly above the median\n",
    "'''\n",
    "sequence_length = 300\n",
    "X_padded = pad_sequences(X, maxlen=sequence_length, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f5c9f294",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Building the\n",
    "train/test split\n",
    "\n",
    "'''\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_padded, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8a141863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - accuracy: 0.6088 - loss: 0.6588 - val_accuracy: 0.8184 - val_loss: 0.4613\n",
      "Epoch 2/10\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 73ms/step - accuracy: 0.8351 - loss: 0.4329 - val_accuracy: 0.8571 - val_loss: 0.4146\n",
      "Epoch 3/10\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 72ms/step - accuracy: 0.8246 - loss: 0.4350 - val_accuracy: 0.6918 - val_loss: 0.5682\n",
      "Epoch 4/10\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 75ms/step - accuracy: 0.6640 - loss: 0.5998 - val_accuracy: 0.7178 - val_loss: 0.5396\n",
      "Epoch 5/10\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 72ms/step - accuracy: 0.7220 - loss: 0.5121 - val_accuracy: 0.7305 - val_loss: 0.5025\n",
      "Epoch 6/10\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 75ms/step - accuracy: 0.7529 - loss: 0.4291 - val_accuracy: 0.7277 - val_loss: 0.5139\n",
      "Epoch 7/10\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 71ms/step - accuracy: 0.7716 - loss: 0.4132 - val_accuracy: 0.9071 - val_loss: 0.3692\n",
      "Epoch 8/10\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 74ms/step - accuracy: 0.7901 - loss: 0.4465 - val_accuracy: 0.8628 - val_loss: 0.3568\n",
      "Epoch 9/10\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 75ms/step - accuracy: 0.9423 - loss: 0.1971 - val_accuracy: 0.9205 - val_loss: 0.2894\n",
      "Epoch 10/10\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 71ms/step - accuracy: 0.9709 - loss: 0.1535 - val_accuracy: 0.9310 - val_loss: 0.2440\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Building the model\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=20000, output_dim=64, input_shape=(sequence_length,)),  # Adjust 'input_dim' as necessary\n",
    "    LSTM(64, return_sequences=False),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1965a117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9313 - loss: 0.2403\n",
      "Validation Accuracy: 93.10%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_val, y_val)\n",
    "print(f\"Validation Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a22c43",
   "metadata": {},
   "source": [
    "# Interpretation and Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3173412e",
   "metadata": {},
   "source": [
    " My model is showing initial strong performance. We can see this by the high accuracy and low loss on the validation set. \n",
    " \n",
    "Something I need to be careful about is potential overfitting.The model might perform better on the training data than on the validation data. We can add some techniques like dropout layers to detect this.\n",
    "\n",
    "I'm going to try some optimization techniques now. The lowest hanging fruit here is Early Stopping.\n",
    "\n",
    "Something else to consider: My input dimensions variable was set to 20,000. This means that the corpus of words\n",
    "only contains 20,000 total different words (tokens). Considering there are 171,000 words in the english dictionary,\n",
    "this might be worth looking into as something I could adjust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "892c7bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 70ms/step - accuracy: 0.9521 - loss: 0.1715 - val_accuracy: 0.9170 - val_loss: 0.2744\n",
      "Epoch 2/10\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 69ms/step - accuracy: 0.9555 - loss: 0.1534 - val_accuracy: 0.9310 - val_loss: 0.2591\n",
      "Epoch 3/10\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 69ms/step - accuracy: 0.9669 - loss: 0.1270 - val_accuracy: 0.9092 - val_loss: 0.2909\n",
      "Epoch 4/10\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 73ms/step - accuracy: 0.9321 - loss: 0.2045 - val_accuracy: 0.9071 - val_loss: 0.2962\n",
      "Epoch 5/10\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 75ms/step - accuracy: 0.9701 - loss: 0.1349 - val_accuracy: 0.9134 - val_loss: 0.2703\n",
      "Epoch 5: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Implementing early stopping\n",
    "and training the model as such\n",
    "'''\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val), callbacks=[early_stopping_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4206fce",
   "metadata": {},
   "source": [
    "Really interesting here. We have a low amount of Epochs (10) but we are not even hitting ten iterations of the model. I have a low patience parameter (3) but I think adjusting Epoch/Patience is not the best move here. I am going to implement something a little different. I am going to try a regularization method to solve the problem of overfitting that we are finding here. I think 97% accuracy is solid but I want this on the test set as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1f7ed6f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_9\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_9\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_19 (\u001b[38;5;33mEmbedding\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │     \u001b[38;5;34m1,280,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_6 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m33,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,313,089</span> (5.01 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,313,089\u001b[0m (5.01 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,313,089</span> (5.01 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,313,089\u001b[0m (5.01 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "Here is the model with dropout\n",
    "\n",
    "'''\n",
    "\n",
    "vocab_size = 20000\n",
    "embedding_dim = 64\n",
    "sequence_length = 300\n",
    "dropout_rate = 0.5\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(sequence_length,)),\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim),\n",
    "    LSTM(64, return_sequences=False),\n",
    "    Dropout(dropout_rate),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9a3157d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 70ms/step - accuracy: 0.5865 - loss: 0.6642 - val_accuracy: 0.6819 - val_loss: 0.6417\n",
      "Epoch 2/30\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 70ms/step - accuracy: 0.6346 - loss: 0.6397 - val_accuracy: 0.6833 - val_loss: 0.5938\n",
      "Epoch 3/30\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 71ms/step - accuracy: 0.6792 - loss: 0.5794 - val_accuracy: 0.7150 - val_loss: 0.5288\n",
      "Epoch 4/30\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 70ms/step - accuracy: 0.7545 - loss: 0.4465 - val_accuracy: 0.7403 - val_loss: 0.5120\n",
      "Epoch 5/30\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 73ms/step - accuracy: 0.7543 - loss: 0.4230 - val_accuracy: 0.7368 - val_loss: 0.4847\n",
      "Epoch 6/30\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 71ms/step - accuracy: 0.8094 - loss: 0.3394 - val_accuracy: 0.8888 - val_loss: 0.4714\n",
      "Epoch 7/30\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 71ms/step - accuracy: 0.8793 - loss: 0.3761 - val_accuracy: 0.7438 - val_loss: 0.5332\n",
      "Epoch 8/30\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 76ms/step - accuracy: 0.7975 - loss: 0.4386 - val_accuracy: 0.8156 - val_loss: 0.4188\n",
      "Epoch 9/30\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 77ms/step - accuracy: 0.8920 - loss: 0.2941 - val_accuracy: 0.9170 - val_loss: 0.2764\n",
      "Epoch 10/30\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 74ms/step - accuracy: 0.9413 - loss: 0.1921 - val_accuracy: 0.9043 - val_loss: 0.2765\n",
      "Epoch 11/30\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 76ms/step - accuracy: 0.9423 - loss: 0.1853 - val_accuracy: 0.9008 - val_loss: 0.2919\n",
      "Epoch 12/30\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 76ms/step - accuracy: 0.9035 - loss: 0.2722 - val_accuracy: 0.9198 - val_loss: 0.2879\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Dropout Model with Early Stopping, I am going to drop an epoch of 30,\n",
    "but I doubt it will reach the 30th iteration.\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train, \n",
    "    epochs=30,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2788f5",
   "metadata": {},
   "source": [
    "I'm at 91% accuracy on the validation set and my overall loss is on the down-trend. This is okay and honestly, I think the best way to improve the accuracy is going to be by getting more clean data, but we don't really have the time or resources to do all of that. Regardless, I'd like to try adjusting my learning rate as a last ditch effort to improve my model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3f70673e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "I am going to try adjusting the learning rate using step decay\n",
    "in my model. This approach will reduce the learning rate\n",
    "by a factor every few epochs.\n",
    "\n",
    "'''\n",
    "\n",
    "initial_learning_rate = 0.001\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "959203a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0d72916f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 68ms/step - accuracy: 0.9294 - loss: 0.2268 - val_accuracy: 0.9050 - val_loss: 0.2950\n",
      "Epoch 2/30\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 68ms/step - accuracy: 0.9311 - loss: 0.2199 - val_accuracy: 0.9191 - val_loss: 0.2505\n",
      "Epoch 3/30\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 68ms/step - accuracy: 0.9452 - loss: 0.1715 - val_accuracy: 0.9163 - val_loss: 0.2688\n",
      "Epoch 4/30\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 68ms/step - accuracy: 0.9429 - loss: 0.1884 - val_accuracy: 0.9120 - val_loss: 0.3185\n",
      "Epoch 5/30\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 69ms/step - accuracy: 0.9444 - loss: 0.1854 - val_accuracy: 0.9331 - val_loss: 0.2383\n",
      "Epoch 6/30\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 69ms/step - accuracy: 0.9555 - loss: 0.1516 - val_accuracy: 0.9268 - val_loss: 0.2502\n",
      "Epoch 7/30\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 71ms/step - accuracy: 0.9017 - loss: 0.2893 - val_accuracy: 0.9240 - val_loss: 0.2537\n",
      "Epoch 8/30\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 71ms/step - accuracy: 0.9502 - loss: 0.1780 - val_accuracy: 0.9296 - val_loss: 0.2470\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=30,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780f97dc",
   "metadata": {},
   "source": [
    "Really happy with this so. 95% accuracy on the training set and then 93% accuracy on the test set. No significant signs of overfitting and I think given the quality and quanitity of our data, this is a very reasonable result."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
